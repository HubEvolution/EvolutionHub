# Webscraper Tool

**Automatische Web-Content-Extraktion mit robots.txt Compliance**

---

## ğŸ¯ Ãœberblick

Das Webscraper-Tool ermÃ¶glicht es Nutzern, strukturierte Inhalte von Webseiten automatisch zu extrahieren. Es respektiert robots.txt-Regeln, implementiert Quota-Limits und bietet eine benutzerfreundliche UI.

### Hauptfeatures

- âœ… **URL-basierte Extraktion**: Titel, Meta-Daten, Text, Links, Bilder
- âœ… **robots.txt Compliance**: Automatische PrÃ¼fung und Einhaltung
- âœ… **Quota-System**: 5 Requests/Tag (Gast), 20 Requests/Tag (User)
- âœ… **Sicherheit**: Rate-Limiting, CSRF-Schutz, Input-Validierung
- âœ… **Internationalisierung**: Deutsch/Englisch
- âœ… **Responsive UI**: Mobile-First Design mit Tailwind CSS

---

## ğŸ“‹ Dokumentation

- **[Specification](spec.md)** - Detaillierte Feature-Spezifikation
- **[Roadmap](roadmap.md)** - 14-Wochen Entwicklungsplan (Phasen 1-4)
- **[Guidelines](guidelines.md)** - Entwicklungsrichtlinien fÃ¼r Claude Code
- **[Testing Strategy](testing.md)** - Test-Pyramide und Quality-Metriken
- **[Implementation Complete](IMPLEMENTATION_COMPLETE.md)** - MVP-Status & Deployment-Guide

---

## ğŸš€ Quick Start

### Lokal testen

```bash
# Dependencies installieren
npm install

# Development-Server starten
npm run dev:remote

# Tool aufrufen
open http://127.0.0.1:8787/tools/webscraper/app
```

### API verwenden

```bash
# URL scrapen
curl -X POST http://127.0.0.1:8787/api/webscraper/extract \
  -H "Content-Type: application/json" \
  -H "X-CSRF-Token: YOUR_TOKEN" \
  -d '{"url": "https://example.com"}'
```

---

## ğŸ— Architektur

```
Webscraper Tool
â”œâ”€â”€ Backend
â”‚   â”œâ”€â”€ Config (src/config/webscraper.ts)
â”‚   â”œâ”€â”€ Types (src/types/webscraper.ts)
â”‚   â”œâ”€â”€ Service (src/lib/services/webscraper-service.ts)
â”‚   â””â”€â”€ API (src/pages/api/webscraper/extract.ts)
â”œâ”€â”€ Frontend
â”‚   â”œâ”€â”€ Island (src/components/tools/webscraper/WebscraperIsland.tsx)
â”‚   â”œâ”€â”€ Form (src/components/tools/webscraper/WebscraperForm.tsx)
â”‚   â”œâ”€â”€ Results (src/components/tools/webscraper/WebscraperResults.tsx)
â”‚   â””â”€â”€ Pages (src/pages/{de,en,}/tools/webscraper/app.astro)
â”œâ”€â”€ Infrastructure
â”‚   â”œâ”€â”€ Migration (migrations/0022_create_scraping_jobs_table.sql)
â”‚   â””â”€â”€ KV (wrangler.toml)
â””â”€â”€ Tests
    â”œâ”€â”€ Unit (tests/unit/services/webscraper-service.test.ts)
    â”œâ”€â”€ Integration (tests/integration/api/webscraper.test.ts)
    â””â”€â”€ E2E (test-suite-v2/tests/webscraper.spec.ts)
```

---

## ğŸ“Š Status

| Component | Status | Coverage |
|-----------|--------|----------|
| Backend Service | âœ… Complete | 100% |
| API Endpoint | âœ… Complete | 100% |
| Frontend UI | âœ… Complete | 100% |
| Unit Tests | âœ… Passing | 10/10 |
| Integration Tests | âœ… Passing | 7/7 |
| E2E Tests | âœ… Passing | 7/7 |
| Build | âœ… Success | 12.89 kB |

---

## ğŸ”§ Konfiguration

### Environment Variables

```bash
# Feature-Flag (default: true)
PUBLIC_WEBSCRAPER_V1=true

# Quotas
WEBSCRAPER_GUEST_LIMIT=5    # Requests/Tag fÃ¼r GÃ¤ste
WEBSCRAPER_USER_LIMIT=20    # Requests/Tag fÃ¼r eingeloggte User

# Timeouts
WEBSCRAPER_TIMEOUT=10000    # Fetch-Timeout in ms

# Limits
WEBSCRAPER_MAX_SIZE=5242880 # Max Response-GrÃ¶ÃŸe (5MB)
```

### KV-Namespaces

```toml
# Development
[[env.development.kv_namespaces]]
binding = "KV_WEBSCRAPER"
id = "webscraper-dev-local"

# Production
[[kv_namespaces]]
binding = "KV_WEBSCRAPER"
id = "webscraper-production"
```

---

## ğŸ“– API-Referenz

### POST /api/webscraper/extract

**Request:**
```json
{
  "url": "https://example.com"
}
```

**Response (Success):**
```json
{
  "success": true,
  "data": {
    "result": {
      "url": "https://example.com",
      "title": "Example Domain",
      "description": "Example description",
      "text": "Page content...",
      "metadata": { "author": "...", "language": "en" },
      "links": ["https://example.com/page1"],
      "images": ["https://example.com/logo.png"],
      "scrapedAt": "2025-10-03T09:00:00.000Z",
      "robotsTxtAllowed": true
    },
    "usage": {
      "used": 1,
      "limit": 5,
      "resetAt": 1728037200000
    }
  }
}
```

**Response (Error):**
```json
{
  "success": false,
  "error": {
    "type": "validation_error|forbidden|server_error",
    "message": "Error description"
  }
}
```

---

## ğŸ§ª Testing

```bash
# Unit-Tests
npm run test:unit:run -- tests/unit/services/webscraper-service.test.ts

# Integration-Tests
npm run test:integration:run -- tests/integration/api/webscraper.test.ts

# E2E-Tests (benÃ¶tigt laufenden Server)
npm run test:e2e -- webscraper.spec.ts
```

---

## ğŸ› Bekannte Limitierungen (MVP)

1. **Kein JavaScript-Rendering** - Nur statisches HTML
2. **Kein Batch-Processing** - Eine URL pro Request
3. **Keine KI-Analyse** - Keine Sentiment/Entity-Erkennung
4. **Kein Monitoring** - Keine geplanten Scraping-Jobs
5. **Keine Bild-Downloads** - Nur URLs, keine Speicherung

â†’ Siehe [Roadmap](roadmap.md) fÃ¼r geplante Features in Phase 2-4

---

## ğŸ“ Best Practices

### FÃ¼r Entwickler

1. **robots.txt respektieren** - Das Tool lehnt automatisch blockierte URLs ab
2. **Quotas beachten** - GÃ¤ste: 5/Tag, User: 20/Tag
3. **Error-Handling** - Nutze strukturierte Fehler-Typen
4. **Testing** - Alle neuen Features mÃ¼ssen getestet sein (â‰¥70% Coverage)

### FÃ¼r Nutzer

1. **Verantwortungsvoller Einsatz** - Keine exzessiven Requests
2. **Rechtliche Compliance** - Urheberrecht und Datenschutz beachten
3. **robots.txt** - Wenn blockiert, respektiere die Entscheidung
4. **Quota-Management** - Bei Limit-Ãœberschreitung warten bis Reset

---

## ğŸ“ Support

- **Issues**: GitHub Issues im Evolution Hub Repository
- **Dokumentation**: Siehe [docs/tools/webscraper/](.)
- **Tests**: Siehe [tests/](../../../tests/)

---

**Version**: 1.0.0-mvp
**Last Updated**: 2025-10-03
**Status**: Production Ready âœ…
